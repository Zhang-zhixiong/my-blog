<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>裸金属问题排查</title>
      <link href="/2022/02/22/openstack-%E8%A3%B8%E9%87%91%E5%B1%9E%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/"/>
      <url>/2022/02/22/openstack-%E8%A3%B8%E9%87%91%E5%B1%9E%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h4 id="1-裸金属正常发放流程"><a href="#1-裸金属正常发放流程" class="headerlink" title="1.   裸金属正常发放流程"></a>1.   裸金属正常发放流程</h4><p>主要日志如下</p><p>日志目录：/var/log/ironic/ironic-conductor.log</p><ol><li>开始部署：<strong>do_node_deploy</strong></li></ol><p>日志：<strong>RPC do_node_deploy called for node</strong></p><p><strong>do_node_deploy Calling event</strong></p><p><code>2018-04-13 11:28:03.154 212706 DEBUG ironic.conductor.manager [req-1311c82c-b51a-4f5e-8718-5bc59b5d6797 - - - - -] **do_node_deploy** Calling event: deploy for node: 946dbfbe-4c02-458b-a3b2-ded74816f262 do_node_deploy /usr/lib/python2.7/site-packages/ironic/conductor/manager.py:599</code></p><ol start="2"><li>创建neutron端口：<strong>creating neutron ports</strong></li></ol><p><code>For node 946dbfbe-4c02-458b-a3b2-ded74816f262, **creating neutron ports** on network 547f8000-e91d-4591-b861-470c07c50c1b using NeutronNetwork network interface. add_ports_to_network /usr/lib/python2.7/site-packages/ironic/common/neutron.py:186</code></p><ol start="3"><li><strong>Build PXE config</strong> </li></ol><p><code>2018-04-13 11:28:11.917 212706 DEBUG ironic.common.pxe_utils [req-1311c82c-b51a-4f5e-8718-5bc59b5d6797 - - - - -] Building PXE config for node 946dbfbe-4c02-458b-a3b2-ded74816f262 create_pxe_config /usr/lib/python2.7/site-packages/ironic/common/pxe_utils.py:195</code></p><ol start="4"><li><strong>Fetching necessary kernel and ramdisk</strong></li></ol><p><code>2018-04-13 11:28:18.082 212706 DEBUG ironic.drivers.modules.pxe [req-1311c82c-b51a-4f5e-8718-5bc59b5d6797 - - - - -] Fetching necessary kernel and ramdisk for node 946dbfbe-4c02-458b-a3b2-ded74816f262 _cache_ramdisk_kernel /usr/lib/python2.7/site-packages/ironic/drivers/modules/pxe.py:303</code></p><ol start="5"><li><strong>Set Boot Device to pxe</strong></li></ol><p><code>2018-04-13 11:28:18.096 212706 DEBUG ironic.common.utils [req-a361c513-d672-4d64-a4c3-72694967fe51 - - - - -] Command stdout is: &quot;Set Boot Device to pxe</code></p><ol start="6"><li>BM上电发心跳：</li></ol><p><code>2018-04-13 11:33:20.043 212706 DEBUG ironic.conductor.manager [req-4c8ecc1f-a4c0-49f5-a21e-315592339bb1 - - - - -] **RPC heartbeat called** **for** **node 946dbfbe**-4c02-458b-a3b2-ded74816f262 heartbeat /usr/lib/python2.7/site-packages/ironic/conductor/manager.py:2588</code></p><ol start="7"><li>准备镜像：</li></ol><p><code>2018-04-13 11:33:30.130 212706 DEBUG ironic.drivers.modules.agent_client [req-4c8ecc1f-a4c0-49f5-a21e-315592339bb1 - - - - -] Preparing image dc4ad588-6688-4466-a074-5305cb8eddbf on **node 946dbfbe**-4c02-458b-a3b2-ded74816f262. prepare_image /usr/lib/python2.7/site-packages/ironic/drivers/modules/agent_client.py:115</code></p><ol start="8"><li>镜像开始时间:</li></ol><p><code>2018-04-13 11:33:58.202 212706 DEBUG ironic.drivers.modules.agent_client [-] Status of agent commands for **node 946dbfbe**-4c02-458b-a3b2-ded74816f262: **prepare_image: result &quot;None&quot;**, error &quot;None&quot; get_commands_status /usr/lib/python2.7/site-packages/ironic/drivers/modules/agent_client.py:107</code></p><ol start="9"><li>镜像结束时间：</li></ol><p><code>2018-04-13 13:26:30.257 212706 DEBUG ironic.drivers.modules.agent_client [-] Status of agent commands for **node 946dbfbe**-4c02-458b-a3b2-ded74816f262: **prepare_image: result &quot;&#123;u&#39;result&#39;:** u&#39;prepare_image: image (dc4ad588-6688-4466-a074-5305cb8eddbf) **written to device /dev/sda** &#39;&#125;&quot;, error &quot;None&quot; get_commands_status /usr/lib/python2.7/site-packages/ironic/drivers/modules/agent_client.py:107</code></p><p>如果在provision过程中错，通过以上日志可准确定位到问题所处的阶段。</p><h4 id="2-挂卷-卸卷类问题"><a href="#2-挂卷-卸卷类问题" class="headerlink" title="2.   挂卷/卸卷类问题"></a>2.   挂卷/卸卷类问题</h4><ol><li>挂卷/卸卷失败，nova-compute日志中提示Connector not found等类似的错误</li></ol><ul><li><p>确认物理连接的连通性，从服务器—交换机—盘机三者之间的物理连接</p></li><li><p>确认管理口是否UP</p></li><li><p>测试Ironic节点到BM的端口，命令：nc –v BM-IP 9999或者telnet BM-IP 9999</p></li><li><p>登录BM，netstat -nlp 9999查看物理机是否监听9999端口</p></li></ul><ol start="2"><li>挂卷/卸卷失败，nova-compute日志显示agent not available等</li></ol><ul><li><p>问题2中的步骤2、3；</p></li><li><p>登录BM，ip a 确认端口状态</p></li><li><p>确认ironic-python-agent是否正常</p></li><li><p>HBA卡端口是否online，命令：<strong>cat sys/class/fc_host/host*/port_state</strong></p></li><li><p>如果以上都没有问题，可以重启下BM的ironic-python-agent</p></li></ul><ol start="3"><li>卸卷长时间显示detaching，cinder/volume日志The specified port is not exist</li></ol><ul><li>确认ERROR的准确位置，如果是cinder.volume.drivers.huawei…，则是华为驱动侧的问题，请协调华为定位。</li></ul><ol start="4"><li>对于可稳定复现的一般类问题定位</li></ol><ul><li><p>卷操作的主要日志记录在volume.log，tailf /var/log/cinder/volume.log 查看报错信息</p></li><li><p>同步tailf /var/log/nova/nova-compute.log，从逻辑上volume.log无ERROR的话，问题肯定出现在上一层的nova侧</p></li></ul><h4 id="3-Nova-boot发放问题"><a href="#3-Nova-boot发放问题" class="headerlink" title="3.   Nova boot发放问题"></a>3.   Nova boot发放问题</h4><ol><li>Inspect失败问题</li></ol><p>​    Inspect过程强依赖和厂商的SDN对接，一定确保对接的准确性。</p><p>​    单个物理机inspect失败，表现为无法获取IP。</p><ul><li>确认ironic节点的dhcpd是否active（inspect获取IP过程使用的是ironic节点的dhcp服务）</li></ul><p>​    <code># systemctl statsu dhcpd</code></p><ul><li>BM侧</li></ul><p>查看ramdisk启动过程，确认每个网口都能被轮训到，并记录下网卡的MAC地址。</p><ul><li>1.3 Ironic控制节点抓包（server端）</li></ul><p>​    <code># tcpdump -i bond0 -s 0 -envv port 67 or port 68</code></p><p>​    开启抓包命令后，立即执行inspect操作，抓取server端的discover和offer报文信息。</p><ul><li>抓包结果分析</li></ul><p>​         i.报文中没有tag标记</p><p>​        很可能BM上联的交换机端口没有加入inspect网络，需要客户或是厂商确认。</p><p>​         ii.只有discover没有offer报文</p><p>​        则说明问题出现在dhcp服务自身。</p><p>​        查看下/etc/dhcp/dhcpd.conf配置的range范围；</p><p>​        查看/var/lib/dhcpd/dhcpd.leases确认下leases条目数。</p><p>​        iii.server端回复offer</p><p>​        则说明包丢在了中间交换机上，需要厂商确认。</p><p>​        注意： discover是广播报文会一直发，而offer回包只有一次（包括request和ACK），一定要注意。</p><ol start="2"><li>批量inspect时失败</li></ol><ul><li>如果单个inspect时没有问题，但是批量inspect时部分失败，检查下配置的dhcp server数量。对于工行部署三个ironic都开启dhcp服务的情况，建议批量inspect的物理机不要超过30个。</li></ul><ol start="3"><li>provision 阶段失败</li></ol><ul><li><p>确认nova节点上neutron-dhcp-agent服务是否active，ironic节点上的dhcpd是否关闭；</p></li><li><p>在nova节点ip netns list查看dhcp（provision网）的名字空间，在此名字空间上ping ironic节点是否通（主要是测试与外部的连通性），命令：ip netns exec <em>qdhcpxxx</em> ping ironic ip</p></li><li><p>进入BM节点的BMC（IPMI地址），查看boot_if及MAC，命令：cat /proc/cmdline查看BOOT_IF的MAC，再根据此MAC查看对应的接口</p></li><li><p>Nova侧确认是否neutron端口已经创建OK</p></li></ul><p>​    <code>neutron port-list | grep MAC # 此MAC是上步骤中的BM主网卡MAC</code></p><ul><li>Nova侧（server端）抓67/68端口报文，tcpdump -s 0 -envv port 67 or port 68，3996（具体以工行规划为准）的tag为provision网VLAN，确认报文的源MAC是否是上个步骤中的MAC，若抓不到discover包侧说明可能包丢在中间的交换机上，需要厂商协助排查。</li></ul><p>​    注意：抓包时先开启抓包命令，再执行provision发放的操作。</p><ul><li>在provision网dhcp命名空间里抓包，确认discover报文到了neutron-dhcp-agent</li></ul><p>​    <code>ip netns exec *qdhcpxxx* tcpdump -s 0 -envv port 67 or port 68</code></p><ul><li>查看br-ex是否收到dhcp请求报文，并将vlan转为3996</li></ul><p>​    命令：<code>ovs-ofctl show br-ex</code></p><ul><li><p>查看br-int上是否有dl_vlan为3996的报文计数，命令ovs-ofctl show br-int</p></li><li><p>server收到discover但是没有offer，参考inspect问题的d）处理。</p></li></ul><p>​    注意： discover是广播报文会一直发，而offer回包只有一次（包括request和ACK），一定要注意。</p><ul><li><p>在BM上手动发起dhcp请求， 命令：dhclient enps0f0，此接口是步骤c中的boot_if口</p></li><li><p>BM节点的boot_if口抓包（client端），tcpdump –i enps0f0 -s 0 -envv port 67 or port 68，查看是否能收到neutron-dhcp-agent发过来的offer报文，如果只有discover但是没有回包，需要交换机厂商协助排查</p></li></ul><ol start="4"><li>nova boot发放失败，nova日志显示 没有host资源、标签不匹配等信息</li></ol><ul><li><p>此类问题主要是控制层面没有合理的调度，主要集中在nova侧的排查</p></li><li><p>首先确认flavor中的标签信息是否跟node里的标签信息一致。Node里的标签信息在properties/capabilities信息中。</p></li><li><p>nova hypervisor-list 先确认hypervisor的信息，nova hypervisor-show 确认下是不是真的资源不够</p></li><li><p>nova hypervisor-servers <hypervisor-id> 查看哪个instance占用了这个hypervisor</p></li><li><p>查看nova-scheduler日志，grep <req-id> /var/log/nova/nova-scheduler.log | grep <node-uuid>，确认此node的关键信息</p></li><li><p>如果以上不能明确定位，可以继续查看nova-compute和neutron server日志，确认是否存在网络的问题，如有AC侧的网络报错，需要协同SDN厂商排查。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openstack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析分布式存储高可用机制</title>
      <link href="/2022/02/20/%E6%B5%85%E6%9E%90%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%BA%E5%88%B6/"/>
      <url>/2022/02/20/%E6%B5%85%E6%9E%90%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;长期以来，大型主机凭借其卓越的性能表现、良好的稳定性以及在单机处理能力方面的强大优势，逐渐发展为金融业数据中心的主流存储产品。但是，随着信息技术的快速发展，金融机构的IT系统也逐步开始向虚拟化与云化转型，并对存储资源建设提出了更高的要求，早已不再是为了满足某一业务系统的特殊目标，期间，传统大型存储设备故障排除难度大、扩展性差、建设周期长等劣势也逐渐凸显。<br>&emsp;&emsp;在此背景下，为更好地支持多租户、高并发等业务场景，分布式存储的概念应运而生，其凭借扩展性、伸缩性极强，以及高效、可靠、高可用等特点，迅速成为金融机构应对网络数据流量快速增长、时间窗口日趋紧张等问题的有力抓手。</p><h4 id="一、什么是分布式存储"><a href="#一、什么是分布式存储" class="headerlink" title="一、什么是分布式存储"></a>一、什么是分布式存储</h4><p>&emsp;&emsp;总体而言，分布式存储基于网络可将分散的存储资源建成一个虚拟的存储资源池，然后再把数据分散存储在多台独立的设备上，因此其不仅拥有极高性能的大并发读写、高可用的故障自动隔离、动态扩展以及自动化运维管理等能力，同时也是构建云计算基础架构的重要组成部分之一。<br>&emsp;&emsp;实践中，分布式存储的常用架构大致可分为两类：一类为使用元数据服务器存储数据位置的架构；另一类为无元数据服务器，通过计算来获取数据位置的架构（如图1所示）。<br><img src="/medias/images/01.png" alt="图1 分布式存储架构示意"><br>其中，对前者而言，如果客户端要从某个文件读取数据，首先需要从元数据获取该文件的位置，然后再从该位置获取具体的数据；后者则是客户端可直接与存储节点通信，并通过设备映射关系计算出其写入数据的具体位置。</p><h4 id="二、分布式存储的特性"><a href="#二、分布式存储的特性" class="headerlink" title="二、分布式存储的特性"></a>二、分布式存储的特性</h4><h6 id="1-弹性扩展"><a href="#1-弹性扩展" class="headerlink" title="1.弹性扩展"></a>1.弹性扩展</h6><p>&emsp;&emsp;因分布式架构无需配置集中接入组件和模块，所以也消除了单个组件和模块的性能瓶颈，同时还能够以服务器为单位进行扩容，实现性能和容量的同步线性增长，甚至平滑扩展至数千个节点。在此基础上，不仅能有效满足前端应用不断扩展的性能需求，而且在线扩容并不会影响应用系统的正常运行，也不会进一步增加存储系统在管理和使用上的复杂性。</p><h6 id="2-高性能"><a href="#2-高性能" class="headerlink" title="2.高性能"></a>2.高性能</h6><p>&emsp;&emsp;分布式存储系统将数据切片后，一般会通过各种智能算法将数据均衡分布在多个节点上的不同硬盘内，从而使前端应用能够在多个节点上进行条带化的并发读写，并可同时向多个硬盘发出I/O请求，从而获取最优的I/O性能。</p><h6 id="3-支持多副本"><a href="#3-支持多副本" class="headerlink" title="3.支持多副本"></a>3.支持多副本</h6><p>&emsp;&emsp;通常，分布式存储会采用多副本备份机制来保证数据的可靠性，即同一份数据可以复制保存为2~3个副本，多副本模式如图2所示。<br><img src="/medias/images/02.png" alt="图2 多副本模式示意"><br>图2中，存储节点1、2、3上的紫色数据块构成了同一份数据的3个副本，当存储节点1的硬盘出现故障时，节点2、3的数据块将继续提供存储服务。同时，该模式还会要求数据写入时采用强一致性保护，即只有当数据均成功写入三份副本后，才算写入操作完成。</p><h6 id="4-故障自愈能力"><a href="#4-故障自愈能力" class="headerlink" title="4.故障自愈能力"></a>4.故障自愈能力</h6><p>&emsp;&emsp;故障自愈能力是指即便个别机器出现故障、损坏等情况，存储服务仍能够保持不中断且一直可用的状态，同时确保故障节点可被快速隔离。此外，自愈能力还包括自动从其他节点的副本中恢复数据，并利用系统中剩余的所有硬盘进行数据重构，进而避免因二次硬件故障带来数据丢失的风险。</p><h4 id="三、落地分布式存储的思路与挑战"><a href="#三、落地分布式存储的思路与挑战" class="headerlink" title="三、落地分布式存储的思路与挑战"></a>三、落地分布式存储的思路与挑战</h4><p>&emsp;&emsp;当前，金融企业最需要的是安全稳定的系统运行环境，而存储数据也必须是高可靠、高可用的，所以要衡量分布式存储能否在金融行业落地，容错能力是其中重要的指标之一。结合高可用机制而言，则是除了传统架构中的备份、双活、多活等策略，还需要系统能针对各类单点故障快速进行识别和处理，进而实现自动化的容错功能。</p><h6 id="1-故障识别能力"><a href="#1-故障识别能力" class="headerlink" title="1.故障识别能力"></a>1.故障识别能力</h6><p>&emsp;&emsp;一般情况下，由于分布式存储的资源都是分摊在各个节点上，因此单节点上的一个服务异常、硬盘故障、网口丢包，都会将影响扩散至整个集群。此外，其他常见的故障类型还包括硬盘I/O卡顿、悬挂，RAID卡、服务器模组、总线等出现故障，以及网络丢包、时延增加等情况。在此背景下，如何在短时间内识别并正确处理众多已知或未知故障，已成为当前提升分布式存储系统高可用能力的关键问题之一。<br>&emsp;&emsp;对此，分布式存储大多是通过建立广泛的故障模型，对各类关键指标信息进行监控，进而评估硬盘健康度/磨损度、服务器CPU/内存状态、节点之间“心跳”是否正常等，以及预测故障发生的可能性并提前预警，以消除潜在隐患。</p><h6 id="2-亚健康检测能力"><a href="#2-亚健康检测能力" class="headerlink" title="2.亚健康检测能力"></a>2.亚健康检测能力</h6><p>（1）慢盘检测<br>&emsp;&emsp;慢盘检测是指在分布式存储系统中对硬盘的I/O响应时间进行监控，如果发现其超过了设定的阈值，那么在一段时间内便会通过读取其他数据副本的方式来快速响应业务，从而减少I/O反应变慢导致的业务卡顿或中断风险。同时，管理服务还会继续对该盘I/O进行检测，当发现持续异常时，则将其标记为Down隔离出集群。<br>（2）I/O悬挂探测<br>&emsp;&emsp;在部分场景下，负责RAID卡或者硬盘内部异常处理的Reset机制会导致部分硬盘上的I/O悬挂，而I/O处于悬挂状态时将无任何信息反馈，从而导致上层主机访问无响应。针对此类I/O悬挂场景，重点是要多方位进行监测控制，即当发现I/O超过几秒无返回时，可对该盘做异常标记，如果后续I/O也不再下发到此硬盘，则主动发起探测I/O对该盘进行检测，以求在快速恢复业务的同时，主动识别故障并进行隔离处理。<br>（3）节点异常处理<br>&emsp;&emsp;在单台服务器的运行过程中，一旦因软硬件故障导致其进入亚健康状态，则数据访问此节点时会出现缓慢、卡顿等情况，此时整个系统的性能都会被影响。针对此类场景，管理系统可通过收集、监控各服务器的关键信息，比如CPU降速、内存不足导致的访问降速等，进行智能分析诊断并定位问题节点。例如，当发现某服务器状态异常时，会将此节点所有硬盘服务快速标记为Down，并对问题节点进行隔离，以避免其拖累整个系统性能。<br>（4）网络亚健康隔离<br>&emsp;&emsp;对于网络来说，网卡降速、丢包/错包率增加等网络故障也会引起I/O的高延时响应，并导致集群写入性能下降甚至写入失败等情况出现。对此，分布式存储系统通过节点自身的网络异常快速检测功能，以及其他节点监控上报等机制，将可快速定位亚健康节点，并对其进行网口切换、节点隔离，以避免网络故障持续影响整个集群性能。</p><h6 id="3-快速处置能力"><a href="#3-快速处置能力" class="headerlink" title="3.快速处置能力"></a>3.快速处置能力</h6><p>&emsp;&emsp;基于金融行业应用系统安全稳定的需求，要求分布式存储对故障的处理速度必须足够快。以Java应用举例，当某个硬盘亚健康出现I/O悬挂时，由于磁盘读写功能异常，将会造成应用虚机上Java的GC日志不能正常写入，并引发整体Java线程停顿，最终出现交易超时、返回报错等情况。对于该问题，可理解为分布式存储系统故障识别处理时间超过了业务所能容忍的最大期限，引起了业务中断的风险，而通过研究发现，10s内的故障影响时间对金融业务基本可实现客户无感知，而这也是当前分布式存储系统在故障处理速度方面提升的主要目标之一。</p><h6 id="4-存储之外的架构优化"><a href="#4-存储之外的架构优化" class="headerlink" title="4.存储之外的架构优化"></a>4.存储之外的架构优化</h6><p>&emsp;&emsp;在金融领域，一般会对数据可靠性及系统可用性提出更高的要求，因此对分布式存储系统而言，除保证自身的高可用机制外，针对其部署高可用架构也是必不可少的环节之一，高可用存储架构如图3所示。<br><img src="/medias/images/03.png" alt="图3 高可用存储架构示意"><br>例如，对于单个存储池的服务器，可进行机柜级容灾部署来保证服务的高可用，即一份数据以三副本的形式，分别保存在机柜1至机柜N中3个机柜上某台设备的某个磁盘上，而机柜数量一般要大于或等于4台，从而保证集群在整个机柜出现故障的情况下，仍能有三个机柜继续执行三副本策略，并正常提供存储服务。<br>此外，一个存储集群通常会包含两个或两个以上的存储池，并可通过云内存储组件来配置不同的后端存储类型A和B，用于对接不同的计算资源主机组，而上层业务虚机则是在A、B主机组中实行高可用部署，以确保单个存储池出现故障时业务不中断。同时，从整个云平台的系统架构来看，存储集群一般采用多集群（主/备）高可用部署，业务同时部署在主、备两个集群，当整个主集群故障不可用时，可快速切换到备集群，从而在分钟级内恢复业务。</p><h4 id="四、未来展望"><a href="#四、未来展望" class="headerlink" title="四、未来展望"></a>四、未来展望</h4><p>&emsp;&emsp;综上所述，分布式存储基于高效率、高可用以及横向扩展、弹性伸缩等优势，充分满足了云计算时代对高性能、高容量、高扩展性的需求，不仅极大地节省了存储资源，且诸如Web服务器、WAS服务器、容器宿主机、文件共享等应用也已经逐步在业内推广。但是，由于分布式存储拓扑结构的复杂性以及软硬件故障的不确定性，其出现故障的概率也大大增加，因此未来在高可用方面仍需进一步提升。<br>&emsp;&emsp;例如，全面覆盖各类已知软硬件故障，提炼实际场景中出现的各种故障模型，以及针对故障进行提前预警、隔离和快速恢复处置等。然而，鉴于IT系统的复杂性，很难保证没有异常场景被遗漏。所以，实践中还需对业务下发的I/O进行实时监控及快速响应，并以最接近业务的方式来实现对异常节点的快速隔离和恢复，进而保证整个存储资源可持续正常对外提供服务，不断提升其在高可用方面的核心能力。</p>]]></content>
      
      
      <categories>
          
          <category> 文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式存储 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
